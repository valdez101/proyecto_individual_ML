{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos los datos a trabajar para el entrenamiento\n",
    "data = pd.read_parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                              0\n",
       "url                             0\n",
       "region                          0\n",
       "region_url                      0\n",
       "price                           0\n",
       "type                            0\n",
       "sqfeet                          0\n",
       "beds                            0\n",
       "baths                           0\n",
       "cats_allowed                    0\n",
       "dogs_allowed                    0\n",
       "smoking_allowed                 0\n",
       "wheelchair_access               0\n",
       "electric_vehicle_charge         0\n",
       "comes_furnished                 0\n",
       "laundry_options             71171\n",
       "parking_options            126682\n",
       "image_url                       0\n",
       "description                     2\n",
       "lat                          1722\n",
       "long                         1722\n",
       "state                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificamos los valores nulos en el archivo\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para no eliminar datos que puedan ser importantes lo cambiamos los archivos none por sin dato en las columnas que tienen el dato none\n",
    "data['laundry_options'] = data['laundry_options'].replace({None: 'Sin dato'})\n",
    "data['parking_options'] = data['parking_options'].replace({None: 'Sin dato'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            0\n",
       "url                           0\n",
       "region                        0\n",
       "region_url                    0\n",
       "price                         0\n",
       "type                          0\n",
       "sqfeet                        0\n",
       "beds                          0\n",
       "baths                         0\n",
       "cats_allowed                  0\n",
       "dogs_allowed                  0\n",
       "smoking_allowed               0\n",
       "wheelchair_access             0\n",
       "electric_vehicle_charge       0\n",
       "comes_furnished               0\n",
       "laundry_options               0\n",
       "parking_options               0\n",
       "image_url                     0\n",
       "description                   2\n",
       "lat                        1722\n",
       "long                       1722\n",
       "state                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos la columna price para discretizar datos y colocarlos en una lista\n",
    "data1 = list(data['price'])\n",
    "category_price = []\n",
    "\n",
    "for i in data1:\n",
    "    if i in range(0,1000):\n",
    "        category_price.append('low')\n",
    "    elif i in range(1000, 2000):\n",
    "        category_price.append('medium')\n",
    "    elif (i >= 2000):\n",
    "        category_price.append('high')\n",
    "    else:\n",
    "        category_price.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asignamos la columna category_price y verificamos la presencia de los valores low en la columna\n",
    "data = data.assign(category_price = category_price)\n",
    "data['category_price'] = np.where(data['category_price'] == 'low', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos las columnas a transformar para el entrenamiento\n",
    "new_data = data[['type', 'sqfeet', 'beds', 'baths', 'cats_allowed', 'dogs_allowed', 'smoking_allowed', 'wheelchair_access', 'electric_vehicle_charge', 'comes_furnished', 'laundry_options', 'parking_options', 'category_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data =pd.get_dummies(new_data, columns=['type', 'laundry_options', 'parking_options'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardamos los datos de caracteristicas a excepcion de la columna a predecir\n",
    "X = new_data.drop('category_price', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardamos los datos que queremos predecir despues del testeo\n",
    "y = new_data['category_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos el train_test_split para el entrenamiento de los datos con un 80% de datos como entrenamiento y el 20% como testeo\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((277183, 35), (69296, 35), (277183,), (69296,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hacemos un shape para ver la cantidad de datos\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('classifier', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'classifier': [LogisticRegression()], \n",
    "                 'classifier__penalty':['l1','l2'], \n",
    "                 'classifier__max_iter':[500, 1000]}, \n",
    "                {'classifier': [MLPClassifier()], \n",
    "                 'classifier__max_iter':[300, 400, 500], \n",
    "                 'classifier__hidden_layer_sizes': [(16, 16, 16), (16, 32, 24), (16, 24, 8)],\n",
    "                 'classifier__activation': ['tanh', 'relu'],\n",
    "                 'classifier__solver': ['sgd', 'adam'],\n",
    "                 'classifier__alpha': [0.0001, 0.05],\n",
    "                 'classifier__learning_rate': ['constate', 'adaptive']},\n",
    "                {'classifier': [KNeighborsClassifier()],\n",
    "                 'classifier__n_neighbors': [4,5,6,7,8,9,10],\n",
    "                 'classifier__leaf_size': [1,3,5,7],\n",
    "                 'classifier__algorithm':['auto', 'kd_tree']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "370 fits failed out of a total of 1020.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "166 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 740, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py\", line 570, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'learning_rate' parameter of MLPClassifier must be a str among {'invscaling', 'adaptive', 'constant'}. Got 'constate' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "133 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 740, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py\", line 570, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'learning_rate' parameter of MLPClassifier must be a str among {'adaptive', 'constant', 'invscaling'}. Got 'constate' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "61 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 740, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py\", line 570, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'learning_rate' parameter of MLPClassifier must be a str among {'adaptive', 'invscaling', 'constant'}. Got 'constate' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.65257272        nan 0.65257272        nan        nan\n",
      "        nan        nan        nan        nan 0.5366166  0.70655848\n",
      " 0.53675009 0.70406918 0.53664907 0.70490255        nan        nan\n",
      "        nan        nan        nan        nan 0.53667072 0.70686877\n",
      " 0.53649394 0.7055447  0.53662382 0.70770934        nan        nan\n",
      "        nan        nan        nan        nan 0.56577455 0.67103706\n",
      " 0.53662382 0.70032072 0.53662021 0.70624463        nan        nan\n",
      "        nan        nan        nan        nan 0.53663464 0.70590189\n",
      " 0.53659135 0.67310369 0.56748047 0.66779676        nan        nan\n",
      "        nan        nan        nan        nan 0.53646869 0.70463198\n",
      " 0.56403922 0.70643945 0.53659135 0.70320695        nan        nan\n",
      "        nan        nan        nan        nan 0.53658774 0.70309145\n",
      " 0.53659856 0.69842662 0.56235032 0.67370979        nan        nan\n",
      "        nan        nan        nan        nan 0.53647951 0.7030878\n",
      " 0.53648312 0.70488809 0.5364759  0.67197448        nan        nan\n",
      "        nan        nan        nan        nan 0.53643261 0.69788559\n",
      " 0.53643261 0.66899876 0.53643982 0.64180332        nan        nan\n",
      "        nan        nan        nan        nan 0.53646147 0.60615564\n",
      " 0.53643261 0.63756419 0.5364759  0.67164978        nan        nan\n",
      "        nan        nan        nan        nan 0.53641457 0.63752453\n",
      " 0.57004615 0.63835431 0.53642539 0.6628432         nan        nan\n",
      "        nan        nan        nan        nan 0.53649394 0.6112033\n",
      " 0.53646147 0.60570765 0.5364759  0.63578198        nan        nan\n",
      "        nan        nan        nan        nan 0.53652641 0.59502228\n",
      " 0.53652641 0.66575536 0.53649394 0.69152156 0.82695548 0.82890727\n",
      " 0.82480888 0.82645762 0.82263343 0.82093059 0.81805522 0.82695548\n",
      " 0.82890727 0.82480888 0.82645762 0.82263343 0.82093059 0.81805522\n",
      " 0.82695548 0.82890727 0.82480888 0.82645762 0.82263343 0.82093059\n",
      " 0.81805522 0.82695548 0.82890727 0.82480888 0.82645762 0.82263343\n",
      " 0.82093059 0.81805522 0.82694827 0.82914176 0.82494237 0.82636382\n",
      " 0.82263704 0.82098109 0.81835466 0.82699878 0.82922114 0.82490268\n",
      " 0.82662358 0.82277413 0.82104242 0.81850619 0.82699156 0.82928608\n",
      " 0.82497484 0.82646844 0.82276691 0.82101356 0.81834023 0.82694105\n",
      " 0.82912734 0.82492072 0.82653699 0.82281742 0.82099552 0.81858916]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='kd_tree', leaf_size=5)\n"
     ]
    }
   ],
   "source": [
    "mejor_modelo = clf.fit(X_train, y_train)\n",
    "print(mejor_modelo.best_estimator_.get_params()['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(algorithm='kd_tree', leaf_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(algorithm=&#x27;kd_tree&#x27;, leaf_size=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(algorithm=&#x27;kd_tree&#x27;, leaf_size=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(algorithm='kd_tree', leaf_size=5)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Precision:  0.82\n",
      "- Recall:  0.82\n",
      "- F-score:  0.82\n",
      "- AUC:  0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_prob = model.predict_proba(X_test)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_test_prob[:,1])\n",
    "print('- Precision: ', round(precision_score(y_test, y_test_pred),2))\n",
    "print('- Recall: ', round(recall_score(y_test, y_test_pred),2))\n",
    "print('- F-score: ', round(f1_score(y_test, y_test_pred),2))\n",
    "print('- AUC: ', round(auc,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajamos el programa con el archivo de testeo\n",
    "data_test = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpiamos los datos del archivo de testeo\n",
    "data_test['laundry_options'] = data_test['laundry_options'].replace({None: 'Sin dato'})\n",
    "data_test['parking_options'] = data_test['parking_options'].replace({None: 'Sin dato'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos las columnas a transformar para el entrenamiento\n",
    "new_data_test = data_test[['type', 'sqfeet', 'beds', 'baths', 'cats_allowed', 'dogs_allowed', 'smoking_allowed', 'wheelchair_access', 'electric_vehicle_charge', 'comes_furnished', 'laundry_options', 'parking_options']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_test =pd.get_dummies(new_data_test, columns=['type', 'laundry_options', 'parking_options'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos dos columnas para tener las mismas cantidades de columnas en el data_test\n",
    "new_data_test['type_assisted living'] = 0\n",
    "new_data_test['type_land'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_test.insert(10,\"type_assisted living\",True)\n",
    "new_data_test.insert(17,\"type_land\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = model.predict(new_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38493</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38494</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38495</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38496</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38497</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38498 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred\n",
       "0         0\n",
       "1         1\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "...     ...\n",
       "38493     0\n",
       "38494     1\n",
       "38495     0\n",
       "38496     0\n",
       "38497     0\n",
       "\n",
       "[38498 rows x 1 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test = pd.DataFrame(res, columns=['pred'])\n",
    "predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportamos el df en un csv sin indice\n",
    "predicted_test.to_csv('valdez101.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7240ea951ceaba11e29eac9de981dee739bdf7955bec6037107e9ed0cae0a328"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
